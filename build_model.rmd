---
title: "NFL Score Prediction Model"
author: "Wade Fuller"
date: "11/21/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, echo = TRUE,
                      message = FALSE, dpi = 180,
                      fig.width = 8, fig.height = 5)
knitr::opts_knit$set(root.dir = "~/Documents/Scripts/R/pred/")
library(tidymodels)
library(doParallel)
game_w_features <- read.csv(paste0("~/Documents/Scripts/R/pred/game_w_features.csv"))
```

## Make the model

# Build a Model
Here, I experiment with the `tidymodels` package to create this model. This is an easy interface to create, train, and tune machine learning models. Ill try to fit an xgboost model for this one. 

```{r}
pre_2021 <- game_w_features %>%
  filter(is.na(home_result) == FALSE, season >= 2007, (week <= 17 & season <= 2020))
post_2021 <- game_w_features %>%
  filter(is.na(home_result) == FALSE, season >= 2007, (week <= 18 & season >= 2021))
gms_w_feats <- rbind(pre_2021,post_2021)
gm_fts_only <- gms_w_feats %>%
  select(-c('X','game_id','gameday','home_team','away_team','away_result','home_points','away_points','home_win','total_points_scored','spread_adj_result','home_spread_cover','away_spread_cover','gametime','season')) %>%
  mutate(week = as.factor(week), weekday = as.factor(weekday), div_game = as.factor(div_game))
set.seed(989)
gm_split <- initial_split(gm_fts_only, strata = home_result)
gm_train <- training(gm_split)
gm_test <- testing(gm_split)
```

Define the various hyperparameters. These will be tuned in the section after this one.

```{r}
xgb_spec <- boost_tree(
  trees = 500
  ,tree_depth = tune()
  ,min_n = tune()
  ,loss_reduction = tune()
  ,sample_size = tune()
  ,mtry = tune()
  ,learn_rate = tune()
  # ,stop_iter = tune() 
  ) %>%
  set_engine("xgboost") %>%
  set_mode("regression")
xgb_spec
```

Define a grid to tune the hyperparameters.

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), gm_train),
  learn_rate(),
  # stop_iter(),
  size = 40
)
xgb_grid
```
Define a workflow for the xgboost model.

```{r}
rfe_model <- rand_forest(mode = 'regression') %>% set_engine('ranger',importance = 'impurity')
model_rec <- gm_train %>%
  recipe(home_result ~ .) %>%
  # step_impute_mean(all_numeric_predictors(),-all_outcomes())%>%
  step_nzv(all_numeric_predictors(),-all_outcomes()) %>%
  step_dummy(all_nominal_predictors(),-all_outcomes(),one_hot = TRUE)
  # step_select_vip(all_predictors(),outcome = 'home_result', model = rfe_model,threshold = .8)
model_rec %>% prep() %>% juice()

c_metrics <- metric_set(rmse,mae,rsq)

xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(model_rec)
```

```{r}
# library(ggforce)
# plot_test_results <- function(recipe, dat = gm_test) {
#   recipe %>%
#     prep() %>%
#     bake(new_data = dat) %>%
#     ggplot() +
#     geom_autopoint(aes(color = home_result), alpha = 0.4, size = 0.5) +
#     geom_autodensity(alpha = .3) +
#     facet_matrix(vars(-home_result), layer.diag = 2) +
#     scale_color_distiller(palette = "BuPu", direction = 1) +
#     labs(color = "Home Result")
# }
# 
# model_rec_trained %>%
#   step_pca(all_numeric_predictors(), num_comp = 20) %>%
#   plot_test_results() +
#   ggtitle("Principal Component Analysis")
# 
# 
# model_rec_trained %>%
#   step_pca(all_numeric_predictors(), num_comp = 10) %>%
#   prep() %>%
#   tidy(number = 6) %>%
#   filter(component %in% paste0("PC", 1:10)) %>%
#   group_by(component) %>%
#   slice_max(abs(value), n = 5) %>%
#   ungroup() %>%
#   ggplot(aes(abs(value), terms, fill = value > 0)) +
#   geom_col(alpha = 0.8) +
#   facet_wrap(vars(component), scales = "free_y") +
#   labs(x = "Contribution to principal component", y = NULL, fill = "Positive?")
```
Define the cross validation pattern. In this case I am using a 10-fold pattern.

```{r}
set.seed(989)
gm_folds <- vfold_cv(gm_train)
gm_folds
```
Enable parallel processing and run the model.
RMSE: 13.37 
RSQ:  .1731
```{r}
doParallel::registerDoParallel()=

set.seed(989)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = gm_folds,
  grid = xgb_grid,
  metrics = c_metrics,
  control = control_grid(save_pred = TRUE)
)

# xgb_res_pca <- tune_grid(
#   xgb_wf_pca,
#   resamples = gm_folds,
#   grid = xgb_grid,
#   metrics = c_metrics,
#   control = control_grid(save_pred = TRUE)
# )
  

# xgb_results <-
#   xgb_wf_set %>%
#   workflow_map(
#     "tune_grid",
#     resamples = gm_folds,
#     grid = xgb_grid,
#     metrics = c_metrics,
#     control = control_grid(save_pred = TRUE)
#   )

# collect_metrics(xgb_res_pca)
# xgb_res_pca %>% filter(id == 'Fold01') %>% unnest(.predictions) %>% summarise(max_pred = max(home_result), min_pred = min(home_result))
xgb_res %>% collect_metrics() %>% filter(.metric == 'rsq', mean >= 0) %>% arrange(-mean)
```
Visualize the hyper parameter grid outcomes for RMSE.

```{r}
 xgb_res %>%
   collect_metrics() %>%
   filter(.metric == "rmse") %>%
   select(mean, mtry:sample_size) %>%
   pivot_longer(mtry:sample_size,
                values_to = "value",
                names_to = "parameter"
   ) %>%
   ggplot(aes(value, mean, color = parameter)) +
   geom_point(alpha = 0.8, show.legend = FALSE) +
   facet_wrap(~parameter, scales = "free_x") +
   labs(x = NULL, y = "Rmse")
```
Determine the best model to use based on the target metric: RMSE

```{r}
this <- show_best(xgb_res, "rmse") %>%
  mutate(rn = row_number()) %>%
  filter(rn == 2)
this <- show_best(xgb_res, "rmse") %>%
  mutate(rn = row_number()) %>%
  filter(rn == 3)
 select_by_one_std_err(xgb_res, "rmse")
 best_rmse <- select_best(xgb_res, "rmse")
 best_rsq <- select_best(xgb_res, "mae")
 best_rmse
 best_rsq
 this

```
Finalize the workflow with our best model.

```{r}
final_xgb_rmse <- finalize_workflow(xgb_wf,best_rmse)
final_xgb_rsq <- finalize_workflow(xgb_wf,best_rsq)
final_xgb_spd <- finalize_workflow(xgb_wf,this)
```

Leverage the finalized workflow to fit training data and identify the most important variables.

```{r}
library(vip)
final_xgb_spd %>%
  fit(data = gm_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "col", num_features = 30, include_type = TRUE, aesthetics = list(fill = "midnightblue", alpha = .9))
ggsave("feat_importance_plot.jpg",path = paste0(wd,"/pred/img"), units = 'in',width = 8, height = 8, device='jpg', dpi=700)
```
Run the model on all data and observe the key metrics.

```{r}
## RMSE: 13.69 // RSQ: .1475
## RMSE: 13.67 // RSQ: .1502 <- added moneylines
## RMSE: 13.69 // RSQ: .1484 <- adjusted pipelines
## RMSE: 13.40 // RSQ: .1597 <- added EPA
## RMSE: 13.68 // RSQ: .1487 <- removed EPA, tuned grid
## RMSE: 13.66 // RSQ: .1771 <- 2007+, RFE, new feats
## RMSE: 13.35 // RSQ: .1400 <- 2007+, -RFE

final_preds_rmse <- last_fit(final_xgb_rmse, gm_split)
final_preds_rsq <- last_fit(final_xgb_rsq, gm_split)
final_preds_spd <- last_fit(final_xgb_spd, gm_split)

collect_metrics(final_preds_rmse)
collect_metrics(final_preds_rsq)
collect_metrics(final_preds_spd)

score_preds_rmse <- final_preds_rmse %>% collect_predictions() %>% rename(rmse_pred = .pred)
score_preds_rsq <- final_preds_rsq %>% collect_predictions() %>% rename(rsq_pred = .pred)
score_preds_spd <- final_preds_spd %>% collect_predictions() %>% rename(spd_pred = .pred)
```

Plot the output of the predictions.

```{r}
score_preds_rmse %>%
  ggplot() +
  geom_density(aes(x = rmse_pred),alpha = 1)
score_preds_rsq %>%
  ggplot() +
  geom_density(aes(x = rsq_pred),alpha = 1)
score_preds_spd %>%
  ggplot() +
  geom_density(aes(x = spd_pred),alpha = 1)
```
Run the model on all of the data.

```{r}
pred_score_model <- fit(final_xgb_rsq, gm_fts_only)
pred_score_model
```
Clean up the data frame and prepare for visualization.

```{r}
model_preds <- predict(pred_score_model, new_data = gm_fts_only)
game_w_preds <- gms_w_feats %>%
  bind_cols(model_preds) %>%
  rename(pred_score = .pred) %>%
  mutate(pred_score_error = pred_score - home_result) %>%
  transmute(
      game_id
     ,pred_spread_adj_winner_m = if_else(pred_score + spread_line > 0,home_team,away_team)
     ,pred_score
     ,home_result
     ,pred_score_error
     ,spread_line
     ,spread_adj_result
     ,home_spread_cover
     ,away_spread_cover
     ,home_team
     ,away_team
     ,season
     ,week
     )
game_pred_df <- game_w_preds %>%
  transmute(
    game_id
    ,home_result
    ,spread_line
    ,pred_score
    ,pred_score_error
    ,spread_error = (-1*spread_line) - home_result
    ,spread_adj_result
    ,spread_adj_winner = if_else(-1*spread_line + home_result > 0,home_team,away_team)
    ,pred_spread_adj_winner = if_else(pred_score + home_result > 0,home_team,away_team)
    ,home_spread_cover = home_spread_cover
    ,away_spread_cover = away_spread_cover
    ,home_team = home_team
    ,away_team = away_team
    ,season
    ,week
  )
summary(game_pred_df)
game_pred_df %>% group_by(season) %>% summarise(avg_error = mean(pred_score_error), abs_error = mean(abs(pred_score_error)))
```

Save the model for later.

```{r}
nfl_predictor_model <- final_preds_rmse$.workflow[[1]]
saveRDS(nfl_predictor_model, paste0("~/Documents/Scripts/R/pred/nfl_predictor_model_",Sys.Date(),".rds"))
saveRDS(nfl_predictor_model, paste0("~/Documents/Scripts/R/pred/nfl_predictor_model.rds"))
```
