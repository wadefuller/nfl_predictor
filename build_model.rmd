---
title: "NFL Score Prediction Model"
author: "Wade Fuller"
date: "11/21/2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(cache = TRUE, echo = TRUE,
                      message = FALSE, dpi = 180,
                      fig.width = 8, fig.height = 5)
knitr::opts_knit$set(root.dir = "~/Projects/R/nfl_predictor/")
library(tidymodels)
library(doParallel)
game_w_features <- read.csv(paste0("~/Projects/R/nfl_predictor/data/game_w_features.csv"))
```

## Make the model

# Build a Model
Here, I experiment with the `tidymodels` package to create this model. This is an easy interface to create, train, and tune machine learning models. Ill try to fit an xgboost model for this one.

```{r}
# Model Configuration
MODEL_CONFIG <- list(
  # Data filters
  min_season = 2008,
  train_split = 0.75,
  
  # Model parameters (if not tuning)
  trees = 1000,  # Max trees with early stopping
  tree_depth = 6,
  learn_rate = 0.1,
  min_n = 10,
  loss_reduction = 0,
  sample_size = 0.8,
  mtry = NULL,  # Will be determined automatically
  stop_iter = 20,  # Early stopping rounds
  
  # Tuning parameters
  tune_model = TRUE,  # Set to FALSE to use fixed parameters above
  cv_folds = 5,  # Number of cross-validation folds
  grid_size = 24,  # Number of parameter combinations to try
  
  # Feature engineering
  remove_season_from_model = TRUE,  # Set to FALSE to include season as predictor
  
  # Random seed
  seed = 989
)

LEAKAGE_VARS <- c(
  "game_id", "gameday","gametime","team", "opposing_team", 
  "points_for","points_against","total_points_scored","pct_points_scored","result","win",
  "home","game_info_gain","win_prob_captured","off_epa","off_pass_epa","off_rush_epa",
  "success_rate", "qb_epa", "home_points","away_points","home_pct_points_scored",
  "away_pct_points_scored","home_win","spread_adjusted_result","spread_cover"
)

data_prep <- function(game_w_features) {
  game_w_features %>%
    filter(
      !is.na(result),
      !is.na(home_moneyline),
      (week <= 17 & season <= 2020) | (week <= 18 & season >= 2021),
      season >= MODEL_CONFIG$min_season
    ) %>%
    mutate(across(c(season, week, weekday, div_game), as.factor)) %>%
    select(-all_of(LEAKAGE_VARS))
}
gms_w_feats <- data_prep(game_w_features)

cat("Dataset shape:", nrow(gms_w_feats), "rows x", ncol(gms_w_feats), "columns\n")
cat("Target variable (home_result) summary:\n")
summary(gms_w_feats$home_result)

```

Set up model training recipe.

```{r}
set.seed(MODEL_CONFIG$seed)
gm_split <- initial_split(gms_w_feats, prop = MODEL_CONFIG$train_split, strata = home_result)
gm_train <- training(gm_split)
gm_test <- testing(gm_split)

basic_recipe <- recipe(home_result ~ ., data = gm_train) %>%
  step_nzv(all_numeric_predictors()) %>%        # Remove zero-variance features
  step_dummy(all_nominal_predictors(), one_hot = TRUE)  # One-hot encode factors
```

Define a grid to tune the hyperparameters.

```{r}
tuned_xgb <- boost_tree(
      trees = MODEL_CONFIG$trees,
      tree_depth = tune(),
      learn_rate = tune(),
      min_n = tune(),
      loss_reduction = tune(),
      sample_size = tune(),
      mtry = tune(),
      stop_iter = MODEL_CONFIG$stop_iter  # Early stopping
    ) %>%
      set_engine("xgboost", validation = 0.2) %>%  # Use 20% for validation in early stopping
      set_mode("regression")

basic_workflow <- workflow() %>%
  add_recipe(basic_recipe) %>%
  add_model(tuned_xgb)
    
```
 
 Train the model and evaluate its performance.

```{r}
cat("\nTuning hyperparameters...\n")
cat("  - CV folds:", MODEL_CONFIG$cv_folds, "\n")
cat("  - Grid size:", MODEL_CONFIG$grid_size, "\n")
cat("  - Early stopping rounds:", MODEL_CONFIG$stop_iter, "\n")

set.seed(MODEL_CONFIG$seed)
cv_folds <- vfold_cv(gm_train, v = MODEL_CONFIG$cv_folds, strata = home_result)

tuning_grid <- grid_latin_hypercube(
    tree_depth(range = c(4, 8)),
    learn_rate(range = c(-2, -0.5)),
    min_n(range = c(5, 20)),
    loss_reduction(range = c(-3, 0)),
    sample_size = sample_prop(range = c(0.6, 0.9)),
    finalize(mtry(), gm_train),
    size = MODEL_CONFIG$grid_size  # Small grid: 12 combinations
  )
cat("  - Hyperparameter combinations:", nrow(tuning_grid), "\n")

library(doParallel)
registerDoParallel()

cat("\n=== Training Model ===\n")
start_time <- Sys.time()

tune_results <- basic_workflow %>%
      tune_grid(
        resamples = cv_folds,
        grid = tuning_grid,
        metrics = metric_set(rmse, mae, rsq),
        control = control_grid(
          save_pred = TRUE,
          verbose = TRUE,
          allow_par = TRUE
        )
      )
end_time <- Sys.time()
training_time <- difftime(end_time, start_time, units = "secs")
cat("Training time:", training_time, "seconds\n")

cat("\n=== Tuning Results ===\n")
best_results <- show_best(tune_results, metric = "rmse", n = 5)
print(best_results)

best_params <- select_best(tune_results, metric = "rmse")
cat("\n=== Best Parameters ===\n")
print(best_params)

final_workflow <- basic_workflow %>%
      finalize_workflow(best_params)

cat("\nFitting final model with best parameters...\n")
fitted_model <- final_workflow %>%
  fit(data = gm_train)

tuning_info <- list(
      tune_results = tune_results,
      best_params = best_params,
      best_metrics = best_results %>% slice(1)
    )

cat("  ✓ Training complete in", round(training_time, 2), "seconds\n")
```

# Save the model for later.

```{r}
evaluate_model <- function(fitted_model, gm_test) {
  
  # Make predictions
  predictions <- fitted_model %>%
    predict(gm_test) %>%
    bind_cols(gm_test %>% select(home_result)) %>%
    mutate(
      residual = home_result - .pred,
      abs_error = abs(residual),
      pred_winner = if_else(.pred > 0, "home", "away"),
      actual_winner = if_else(home_result > 0, "home", "away"),
      correct = pred_winner == actual_winner
    )
  
  # Calculate metrics
  mae <- predictions %>% mae(truth = home_result, estimate = .pred)
  rmse <- predictions %>% rmse(truth = home_result, estimate = .pred)
  rsq <- predictions %>% rsq(truth = home_result, estimate = .pred)
  accuracy <- mean(predictions$correct)
  
  # Feature importance
  importance <- fitted_model %>%
    extract_fit_parsnip() %>%
    vip::vi() %>%
    arrange(desc(Importance))
  
  list(
    predictions = predictions,
    metrics = tibble(
      mae = mae$.estimate,
      rmse = rmse$.estimate,
      rsq = rsq$.estimate,
      accuracy = accuracy
    ),
    importance = importance
  )
}

make_predictions <- function(fitted_model, game_w_features) {
  # Make predictions
  all_predictions <- fitted_model %>%
    predict(gms_w_feats) %>%
    bind_cols(gms_w_feats %>% select(home_result,everything())) %>%
    mutate(
      residual = home_result - .pred,
      abs_error = abs(residual),
      pred_winner = if_else(.pred > 0, "home", "away"),
      actual_winner = if_else(home_result > 0, "home", "away"),
      correct = pred_winner == actual_winner
    )
}



print_evaluation <- function(evaluation) {
  cat("\n=== Model Performance ===\n")
  cat("MAE:      ", round(evaluation$metrics$mae, 3), "\n")
  cat("RMSE:     ", round(evaluation$metrics$rmse, 3), "\n")
  cat("R²:       ", round(evaluation$metrics$rsq, 3), "\n")
  cat("Accuracy: ", round(evaluation$metrics$accuracy * 100, 2), "%\n")
  
  cat("\n=== Top 10 Features ===\n")
  print(evaluation$importance %>% head(10))
}

evaluation <- evaluate_model(fitted_model, gm_test)
  
print_evaluation(evaluation)

all_preds <- make_predictions(fitted_model,gms_w_feats)

model_package <- list(
    workflow = fitted_model,
    config = MODEL_CONFIG,
    metrics = evaluation$metrics,
    feature_importance = evaluation$importance,
    training_time = training_time,
    train_rows = nrow(gm_train),
    test_rows = nrow(gm_test),
    trained_date = Sys.Date(),
    tuning_info = tuning_info  # NULL if not tuned
  )

fitted_model %>%
  extract_fit_parsnip() %>%
  vip::vip(num_features = 20, aesthetics = list(fill = "steelblue")) +
  labs(title = "Top 20 Most Important Features") +
  theme_minimal()

all_preds %>%
  ggplot(aes(x = home_result, y = .pred)) +
  geom_point(alpha = 0.3, color = "steelblue") +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "red") +
  geom_smooth(method = "lm", se = TRUE, color = "darkblue") +
  labs(
    title = "Predicted vs Actual Home Result",
    x = "Actual Point Differential",
    y = "Predicted Point Differential"
  ) +
  theme_minimal()

# Calculate metrics by season
season_metrics <- all_preds %>%
  group_by(season) %>%
  summarise(
    games = n(),
    mae = mean(abs_error),
    rmse = sqrt(mean(residual^2)),
    accuracy = mean(correct),
    spread_mae = mean(abs(spread_line - .pred)),
    .groups = 'drop'
  )

print(season_metrics)

season_metrics %>%
  select(season, mae, spread_mae) %>%
  pivot_longer(cols = c(mae, spread_mae), 
               names_to = "metric", 
               values_to = "value") %>%
  mutate(metric = recode(metric, 
                         mae = "Model MAE",
                         spread_mae = "Spread MAE")) %>%
  ggplot(aes(x = season, y = value, color = metric, group = metric)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_color_manual(values = c("Model MAE" = "steelblue", 
                                 "Spread MAE" = "coral")) +
  labs(
    title = "Model Performance vs Market Spread by Season",
    subtitle = "Lower is better",
    x = "Season",
    y = "Mean Absolute Error (points)",
    color = NULL
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    plot.title = element_text(face = "bold", size = 14),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

season_metrics %>%
  ggplot(aes(x = season, y = accuracy)) +
  geom_line(color = "steelblue", size = 1.2) +
  geom_point(color = "steelblue", size = 3) +
  scale_y_continuous(labels = scales::percent, limits = c(0.0, 0.8)) +
  labs(
    title = "Winner Prediction Accuracy by Season",
    x = "Season",
    y = "Accuracy"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


# MAE:       10.404 
# RMSE:      13.547 
# R²:        0.148 
# Accuracy:  64.48 %
```
```{r save_model, eval=FALSE}
saveRDS(model_package,paste0("~/Projects/R/nfl_predictor/models/nfl_score_predictor_model_",Sys.Date(),".rds"))
cat("✓ Model saved successfully\n")
```